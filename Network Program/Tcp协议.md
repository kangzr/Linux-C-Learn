### TCP细节

TCP 是一个有连接的、可靠的、带流量控制和拥塞控制的端到端的协议

建立连接、数据传输、断开连接的流程图

![tcp_shake](..\pic\tcp_shake.png)

Tcp状态图

![tcp_state_1](..\pic\tcp_state_1.png)



#### 一. 建立连接：

**建立连接进行初始化的目的**：分配资源、初始化序列号（通知peer对端我的初始序列号是多少）

1. 主动端首先发送一个SYN包告诉被动端我的初始化序列号是X（主动方状态：SYN_SEND，被动方状态：LISTEN）
2. 被动端收到SYN包后回复ACK确认包表明我已收到，并告知自己的初始化序列号Y；将ACK+SYN同时发送给主动端（主动方状态：SYN_RCVD）
3. 被动端收到回复主动端一个ACK确认包，告诉其我已收到（主/被：ESTABLISED）

握手不一定都是3次，**也可能出现四次握手建立连接**，比如当peer两端同时发起SYN来建立连接时，（有些实现不支持）

![tcp_4_shake](D:\MyGit\Linux-C-Learn\pic\tcp_4_shake.png)



**Q：TCP连接的初始化序列号（ISN, Inital Sequence Number）X，Y能否写死固定？**

不能。如果ISN固定为1，双方建立连接后主动方给被动发送了10个包，这10个包不知为何被链路上的路由器缓存了（路由器会毫无征兆的缓存或丢弃任何数据包），这时主动方挂掉了，然后其又以同样端口重新连上被动方，这时候被动方收到了路由器缓存的10个数据包，并回复给主动方，就会完全错乱了。

RFC793中，建议ISN同时钟绑定，每4微妙对ISN加一，直到超时2^32，又从0开始，因此需要4个小时才会产生ISN的回绕。这种递增的方式容易让攻击者猜到TCP连接的ISN，因此大多在一个基准值的基础上进行随机加

**Q：初始化连接的SYN超时问题：主动方发送一个SYN包给被动方后就挂了，此时连接处于什么状态？会超时吗？**

被动方收到主动方发送的SYN包后，该连接便会放入SYN半连接队列，如果主动方挂掉，这个连接**会一致占用SYN半连接队列的资源**，如果大量这种连接会将SYN半连接队列的资源耗尽（DDos攻击，Syn Flood），让正常连接无法得到处理。因此需要设置超时机制，目前Linux默认会进行5次重发SYN-Ack包，重试间隔为1s，2s，4s，8s，16s，第5ci发送还需要等32s才知道是否超时，因此需要63s才能断开连接。

SYN 超时需要 63 秒，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的 SYN 包给 Server(俗称 SYN flood 攻击)，用于耗尽 Server 的 SYN 队列。对于应对 SYN 过多的问题，linux 提供了几个 TCP 参数：`tcp_syncookies`、`tcp_synack_retries`、`tcp_max_syn_backlog`、`tcp_abort_on_overflow` 来调整应对。



#### 二. 断开连接

**TCP断开连接的目的**：回收资源，终止数据传输；由于TCP是全双工的，需要Peer两端各自拆除自己通向Peer对端方向的通信信道；

1. 主动方发送一个FIN给被动方，表示没有数据传输，要求断开连接；（主：FIN_WAIT_1）
2. 被动方收到FIN包后，回复一个ACK确认包（被：CLOSE_WAIT）；主动方收到ACK后进入FIN_WAIT_2状态
3. 被动方没有数据再发送后，也发送一个FIN包给主动方，（主：LAST_ACK）
4. 主动方收到后，回复ACK确认（主：TIME_WAIT状态）



**Q：TCP的Pee双方同时发起断开连接的FIN包会怎样，TCP状态如何转移？**

双方同时发送FIN包后两者都进入FIN_WAIT_1状态，如果一方在FIN_WAIT_1状态如果收到FIN数据包，其在收到对端Peer全部的数据后，响应一个ACK，然后自己进入CLOSEING状态，在CLOSEING状态下收到自己发送的FIN的ACK包后，进入TIME_WAIT状态。因此双方可能出现完全一样的状态`FIN_WAIT1 --> CLOSEING-->TIME_WAIT`。也就是双方可能同时进入TIME_WAIT状态。如图：

![tcp_same_wait](D:\MyGit\Linux-C-Learn\pic\tcp_same_wait.png)

**Q：四次挥手能不能变成三次？**

可以。被动方的ACK确认可否跟FIN包一起发送，这样四次挥手变成三次。即如果被动方没有数据在发送的情况。



#### 三. TIME_WAIT状态

**1.Peer双端，哪一端会进入TIME_WAIT，为何？**

可能主动发送FIN的端会进入TIME_WAIT，也可能双端同时进入TIME_WAIT（双方同时发送FIN）；

如果主动方没有TIME_WAIT状态，发送对FIN的ACK包后就关掉了，而ACK包在路由过程中丢掉了，被动方没有收到，就会超时重传FIN，这时候主动方已经关闭了，被动方便无法正常关闭连接；所以需要有TIME_WAIT状态以便能够重发丢掉的被动方关闭方的FIN的ACK包。

**2. TIME_WAIT状态用来解决或避免什么问题？**

- 如果没有TIME_WAIT状态，被动方重传FIN包时，这个连接已经不存在于主动关闭方了，因此无法识别FIN包，协议栈会认为对方疯了，回复一个RST包给被动方。
- 防止已经断开的连接1中的链路中残留FIN包终止掉新的连接2，如果重用了连接1的所有5元素：（源IP，目的IP，TCP，源端口，目的端口）。概率很低但是会出现，

**3. TIME_WAIT会带来哪些问题？**

一个连接进入TIME_WAIT状态后需要等待2*MSL（数据包一来一回时间），才能断开连接释放占用的资源。会造成一下问题：

- 作为服务器，短时间内存关闭了大量的client连接，就会造成服务器上出现大量的TIME_WAIT连接，占据大量的tuple，严重消耗服务器资源
- 作为客户端，短时间内大量的短连接，会大量消耗Client机器的端口，毕竟端口只有65535个，端口被耗尽，便无法发起新的连接

首选 UNIX 域套接字而不是 TCP。如何解决或缓解TIME_WAIT问题？可以进行TIME_WAIT的快速回收和重用来缓解。

**4. T.IME_WAIT的快速回收和重用**

**(1) TIME_WAIT快速回收**

Linux下开启TIME_WAIT快速回收需要同时打开`tcp_tw_recycle`和`tcp_timestamps`(默认打开)两个选项。

Linux下快速回收的时间为3.5 * RTO（Retransmission Timeout）。但是，开启快速回收TIME_WAIT，可能造成同时满足以下三种情况导致新连接被拒绝：

- 来自同一对端peer的TCP包携带了时间戳
- 之前某一台peer机器的某个tcp数据在MSL时间内到过本server
- Peer机器新连接的时间戳小于上次TCP到来的时间戳，且差值大于重放窗口戳(TCP_PAWS_WINDOW)。

>  NAT：允许一个整体机构以一个公用IP（Internet Protocol）地址出现在Internet上，内部私有网络地址（IP地址）翻译成合法网络IP地址的技术
>
> NAT分为三种：静态NAT(Static NAT)、动态地址NAT(Pooled NAT)、网络地址端口转换NAPT（Port-Level NAT）

但是需要考虑**NAT（Network Address Translation网络地址转换）**：在一个 NAT 后面的所有 Peer 机器在 Server 看来都是一个机器，NAT 后面的那么多 Peer 机器的系统时间戳很可能不一致，有些快，有些慢。

即同一个 IP 在 TCP_PAWS_MSL（60s） 时间内，如果本次的连接的 timestamp 滞后于前一个连接的 timestamp, 这个连接就会丢弃。

公司通过同一个 WIFI 经过路由器 NAT 去访问服务，在服务端看来就是同一个公网 IP， 在上网高峰期，由于网络的拥塞，**可能会导致先发出的 TCP 包后到达服务器的情况，导致服务器不响应**。

**(2) TIME_WAIT重用**

Linux比较完美的实现了TIME_WAIT重用问题，只要满足下面两个点中的一点，一个TW状态的四元组(即一个socket连接)可以重新被新到来的SYN连接使用

- 新连接SYN告知的初始序列号比TIME_WAIT老链接的末序列号大
- 如果开启了`tcp_timestamps`，并且新到来的连接的时间戳比老连接的时间戳大

要同时开启`tcp_tw_reuse`选项和`tcp_timestamps`选项才可开启TIME_WAIT重用，还有一个条件：重用 TIME_WAIT 的条件是收到最后一个包后超过 1s。

重用没有解决TIME_WAIT造成的资源消耗问题；

时间戳重用 TIME_WAIT 连接的机制的前提是 IP 地址唯一性，得出新请求发起自同一台机器，但是如果是 NAT 环境下就不能这样保证了，于是在 NAT 环境下，TIME_WAIT 重用还是有风险的。

**区别tcp_tw_reuse 和 SO_REUSEADDR 选项**，认为是相关的一个东西，其实他们是两个完全不同的东西，可以说两个半毛钱关系都没。tcp_tw_reuse 是内核选项，而 SO_REUSEADDR 用户态的选项，使用 SO_REUSEADDR 是告诉内核，如果端口忙，但 TCP 状态位于 TIME_WAIT，可以重用端口。如果端口忙，而 TCP 状态位于其他状态，重用端口时依旧得到一个错误信息，指明 Address already in use”

**(3) 清掉TIME_WAIT的HACK技术**

- 修改tcp_max_tw_buckets，控制并发的TIME_WAIT数量。
- 利用RST包从外部清掉TIME_WAIT连接。内核收到 RST 将会产生一个错误并终止该连接，即所谓RST攻击。

#### 四. TCP可靠性与流量控制

Q：TCP的可靠传输时确认号来实现的，那么TCP的确认机制时怎样的呢？是收到一个包就马上确认，还是可以延迟确认？

Q：假如发送一个包，一直都没收到确认呢？什么时候重传呢？超时机制的怎样的？

Q：TCP 两端 Peer 的处理能力不对等的时候，比如发送方处理能力很强，接收方处理能力很弱，这样发送方是否能够不管接收方死活狂发数据呢？如果不能，流量控制机制的如何的？

Q：TCP 是端到端的协议，也就是 TCP 对端 Peer 只看到对方，看不到网络上的其他点，那么 TCP 的两端怎么对网络情况做出反映呢？发生拥塞的时候，拥塞控制机制是如何的？



**(1)延迟确认机制**

确认机制：累积式，也。

出现大量确认号ACK：

- 确认号本身是不包含数据的分段，
- 对每个包进行确认

大量的确认号消耗了大量的带宽，使得网络利用率下降。针对以上两种情况，采用以下方式来尽量减少确认号数量，进而提高网络利用率

- ACK包可以同数据一起捎带传输，TCP协议头中总使包含确认号的。
- 采用累计式确认机制：即确认号X的确认指示的是所有X之前但不包括X的数据已经收到，而不是对所有顺序包进行ACK
- 采用延迟确认，ACK在收到数据后并不马上回复，而是延迟一段时间



**(2) TCP重传机制以及重传的超时计算**

**i) TCP重传超时计算**

发送的包一直没有收到ACK确认，怎么办？

超时重传，这个timer如何设置，太短的话ACK还在路上，造成重传浪费，过多重传会造成网络拥塞，进一步加剧数据丢失；太长的话，效率太差。

因此，应该根据网络状况的变化，采用一个算法去计算重传的时间。根据RTT(Round Trip Time)来设置RTO(Retransmission TimeOut)；

**Ii) TCP重传机制**

两种重传方案：

- 只重传丢失的包：按需重传，能节省带宽，但是重传比较慢，需要一个个处理重传。
- 重传第一个丢失的包后面所有的包：重传较快，但是重传了很多不必要的包，浪费带宽，在出现丢包时，一般时网络阻塞，大量重传又进一步加剧拥塞。

快速重传算法(Fast Retransmit)，连续收到3次相同确认号的ACK，就进行重传。这个算法基于一种假设，连续收到3个相同的ACk，说明当前网络状况变好，可重传。可以解决timeout问题，但是没有解决重传一个还是多个的问题。RFC2018提出Selective Acknowledement机制，可以告知接收端缺了哪些。



**(3) TCP流量控制**

TCP窗口是一个16bit字段（2个字节），其最大为(2^16=65535)个字节。还有一个TCP窗口扩大因子，来用大窗口大小。

这个窗口是接收端告诉发送端自己还有多少缓冲区可以接收数据，发送端根据这个窗口来调整发送数据的速率，进而达到端对端的流量控制。

**i) 发送端如何方便知道自己哪些包可发，哪些不能?**

**采用TCP滑动窗口**：发送端维护一个跟接收端大小一样的发送窗口即可，窗口内的可发，窗口外的不可，窗口在发送序列上不断后移。如下图：

![tcp_window](..\pic\tcp_window.png)

从上图可看出，TCP发送端数据可分为4类，其中2，3两部分合起来称之为发送窗口

1. 已经发送并得到接收端ACK的
2. 已经发送但未收到接收端ACk的
3. 未发送但允许发送的（接收方还有空间）
4. 未发送且不允许发送的（接收方没空间）

下图演示窗口滑动情况，收到36的ACK后，窗口向后滑动5个Byte

![tcp_slide](..\pic\tcp_slide.png)



**ii) 如果接收端通知一个0窗口给发送端，发送端能不能发送数据？如果不发送数据，那一直等待一个非0窗口？如果接收端一直不通知？**

接收端接收控制，如下图所示：

![tcp_windows_control](..\pic\tcp_windows_control.png)

接收端通知一个zero窗口时，发送端不能发数据，如果一直等待，太受限于接收端，因此TCP使用Zero Window Probe技术，缩写为ZWP，发送端在窗口变成0后，会发ZWP的包给接收方，来探测目前接收端窗口大小，一般设置成3次，如果3次后还是0，有的TCP会RST掉这个连接。

**DDos攻击点**：攻击者可以在和Server建立好连接后，就向Server通知一个0窗口，然后Server端只能等待ZWP，与攻击者发大量的这样的请求，耗尽服务端资源。



**iii) 如果接收端处理能力很慢，这样接收端的窗口很快被填满，然后接收处理完几个字节，腾出几个字节的窗口后，通知发送端，这个时候发送端马上就发送几个字节给接收端吗？发送的话会不会太浪费了，就像一艘万吨油轮只装上几斤的油就开去目的地一样。对于发送端产生数据的能力很弱也一样，如果发送端慢吞吞产生几个字节的数据要发送，这个时候该不该立即发送呢？还是累积多点在发送？**

本质是为了避免发送大量小包的问题(Silly Window Syndrome糊涂窗口综合征)，造成这个问题原因有二：

1. 接收端一直在通知一个小的窗口
2. 发送端本身问题，一直在发送小宝

针对上述两个原因，可有两种解决思路：

1. 接收端不通知小窗口：Nagle算法(包长度>MSS，允许发送，包含FIN，允许发送，设置TCP_NODELAY，允许发送，超时，立即发送)
2. 发送端积累一下数据再发送



**(4) TCP拥塞控制**

本制：网络拥塞原因是大家都想独享整个网络资源，对TCP，端对端的流量控制必然导致网络阻塞，因为TCP只看到对端的接收空间大小，无法知道链路上的容量，只要双方的处理能力强，就可以很大速率发包，于是链路很快出现拥堵，进而引起大量的丢包，丢包又引起大量重传，，进一步加剧链路的拥塞。

另外一个拥塞的因素是链路上的转发节点，例如路由器，再好的路由器只要接入网络，总是会拉低网络的总带宽，如果在路由器节点上出现处理瓶颈，那么就很容易出现拥塞。

TCP 看不到网络的状况，那么拥塞控制是必须的并且需要采用试探性的方式来控制拥塞，于是拥塞控制要完成两个任务：公平性和拥塞过后的恢复



Duplicate Ack（重复ACK），同时满足以下条件：

- 接收ACK的那端已经发出了一些还没被ACK的数据包
- 该ACK没有捎带数据
- 该ACK的SYN和FIN位都是0
- 该ACK的确认号等于接收ACK那端已经收到的ACK的最大号
- 该ACk通知的窗口接收该ACK的那端上一个手打的ACK的窗口



TCP拥塞控制算法：Reno算法, 主要包括四个部分

1. 慢热启动算法 - Slow Start
2. 拥塞避免算法 - Congestion Avoidance
3. 快速重传 - Fast Retransimit
4. 快速恢复算法 - Fast Recovery

在一个发送端通知的接收窗口(rwnd)的基础上，加上一个拥塞窗口(cwnd)控制。因此发送端真正的窗口=min(rwnd, cwnd)。

**i) 慢热启动算法**

慢启动体现了一个试探的过程，刚接入网络的时候先发包慢点，探测一下网络情况，然后在慢慢提速。不要一上来就拼命发包，这样很容易造成链路的拥堵，出现拥堵了在想到要降速来缓解拥堵这就有点成本高了，毕竟无数的先例告诫我们先污染后治理的成本是很高的。

- 连接建好初始化cwnd = N
- 每接收一个ACK，++cwnd，线性上升
- 每过一个RTT，cwnd = cwnd * 2, 指数上升
- 慢启动门限ssthresh(slow start threadhold)，当cwnd >= ssthresh，进入拥塞避免算法

**ii) 拥塞避免算法**

- 每收到一个ACK，cwnd=(cwnd + 1 / cwnd) * MSS个字节
- 每经过一个RTT时长，cwnd++

TCP 是看不到网络的整体状况的，那么 TCP 认为网络拥塞的主要依据是它重传了报文段。

TCP两种重传

1）出现RTO超时，重传数据包，TCP认为拥塞的可能性很大，反应非常强烈

- 调整门限ssthresh的值为当前cwnd的1/2；
- reset自己的cwnd为1
- 重新进入慢启动过程

2) 在RTO超时前，收到3个duplicate ACK进行重传数据包。到 3 个冗余 ACK 后说明确实有中间的分段丢失，然而后面的分段确实到达了接收端，因为这样才会发送冗余 ACK，这一般是路由器故障或者轻度拥塞或者其它不太严重的原因引起的，因此此时拥塞窗口缩小的幅度就不能太大，此时进入快速重传。

**iii) 快速重传**

- 调整门限sthresh的值为当前cwnd的1/2
- 将cwnd设置为新的ssthresh
- 重新进入拥塞避免阶段

在快速重传的时候，一般网络只是轻微拥堵，在进入拥塞避免后，cwnd 恢复的比较慢。针对这个，“快速恢复”算法被添加进来，当收到 3 个冗余 ACK 时，TCP 最后的[3]步骤进入的不是拥塞避免阶段，而是快速恢复阶段。

**iv) 快速恢复算法**

快速恢复的思想是“数据包守恒”原则，即带宽不变的情况下，在网络同一时刻能容纳数据包数量是恒定的。当“老”数据包离开了网络后，就能向网络中发送一个“新”的数据包。既然已经收到了 3 个冗余 ACK，说明有三个数据分段已经到达了接收端，既然三个分段已经离开了网络，那么就是说可以在发送 3 个分段了。于是只要发送方收到一个冗余的 ACK，于是 cwnd 加 1 个 MSS。快速恢复步骤如下(在进入快速恢复前，cwnd 和 sshthresh 已被更新为：sshthresh = cwnd /2，cwnd = sshthresh)：

- 把 cwnd 设置为 ssthresh 的值加 3，重传 Duplicated ACKs 指定的数据包
- 果再收到 duplicated Acks，那么 cwnd = cwnd +1
- 如果收到新的 ACK，而非 duplicated Ack，那么将 cwnd 重新设置为【3】中 1）的 sshthresh 的值。然后进入拥塞避免状态。

#### 五. P2P理论上的加速比

传统的 C/S 模式传输文件，在跑满 Client 带宽的情况下传输一个文件需要耗时 FS/BW，如果有 n 个客户端需要下载文件，那么总耗时是 n*(FS/BW)，当然啦，这并不一定是串行传输，可以并行来传输的，这样总耗时也就是 FS/BW 了，但是这需要服务器的带宽是 n 个 client 带宽的总和 n*BW。C/S 模式一个明显的缺点是服务要传输一个文件 n 次，这样对服务器的性能和带宽带来比较大的压力，我可以换下思路，服务器将文件传给其中一个 Client 后，让这些互联的 Client 自己来交互那个文件，那服务器的压力就减少很多了。这就是 P2P 网络的好处，P2P 利用各个节点间的互联，提倡“人人为我，我为人人”。

#### 六. 系统调用backlog参数指什么

Linux的协议栈维护TCP联机的两个连接队列：

1. SYN半连接队列:

   Server端收到client的SYN包并回复SYN+ACK后，该连接的信息会被放入一个队列，即为SYN半连接队列（此使TCP处于非同步状态）；

   

   对于 SYN 半连接队列的大小是由（/proc/sys/net/ipv4/tcp_max_syn_backlog）这个内核参数控制的，有些内核似乎也受 listen 的 backlog 参数影响，取得是两个值的最小值。当这个队列满了，Server 会丢弃新来的 SYN 包，而 Client 端在多次重发 SYN 包得不到响应而返回（connection time out）错误。但是，当 Server 端开启了 syncookies，那么 SYN 半连接队列就没有逻辑上的最大值了，并且/proc/sys/net/ipv4/tcp_max_syn_backlog 设置的值也会被忽略。

   

2. Accept全连接队列:

   Server端收到SYN+ACK包的ACK后，会将连接信息从SYN半连接队列移到另一个队列，即为Accept全连接队列（TCP连接建立，三次握手完成）

   accept 连接队列的大小是由 backlog 参数和（/proc/sys/net/core/somaxconn）内核参数共同决定，取值为两个中的最小值。当 accept 连接队列满了，协议栈的行为根据（/proc/sys/net/ipv4/tcp_abort_on_overflow）内核参数而定。如果 tcp_abort_on_overflow=1，server 在收到 SYN_ACK 的 ACK 包后，协议栈会丢弃该连接并回复 RST 包给对端，这个是 Client 会出现(connection reset by peer)错误。如果 tcp_abort_on_overflow=0，server 在收到 SYN_ACK 的 ACK 包后，直接丢弃该 ACK 包。这个时候 Client 认为连接已经建立了，一直在等 Server 的数据，直到超时出现 read timeout 错误。

   

用户进程调用 accept()系统调用后，该连接信息就会从全连接队列中的队列中移走。



有些认为 backlog 指的是[1]和[2]两个队列的和。而有些则认为是 backlog 指的是[2]的大小。其实，这两个说法都对，在 linux kernel 2.2 之前 backlog 指的是[1]和[2]两个队列的和。

##### TCP短连接和长连接的区别

**短连接**：Client向Serer发送消息，Server回应Client，一次读写完成，双方都可发起close操作。

**长连接**：Client与Server完成一次读写后，双方不会主动关闭连接。

#### TCP粘包、拆包及解决方法



**Q：为什么TCP有粘包和拆包，而不说UDP？**

UDP 是基于报文发送的，**UDP首部采用了 16bit 来指示 UDP 数据报文的长度**，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。

而 TCP 是**基于字节流**的，虽然应用层和 TCP 传输层之间的数据交互是大小不等的数据块，但是 TCP 并没有把这些数据块区分边界，仅仅是**一连串没有结构的字节流**；另外从 TCP 的帧结构也可以看出，在 **TCP 的首部没有表示数据长度的字段**，基于上面两点，在使用 TCP 传输数据时，才有粘包或者拆包现象发生的可能。



**Q：什么是粘包、拆包？**

假设 Client 向 Server 连续发送了两个数据包，用 packet1 和 packet2 来表示，那么服务端收到的数据可以分为三种情况，现列举如下：

**第一种情况**，接收端正常收到两个数据包，即没有发生拆包和粘包的现象。

![正常情况](..\pic\tcp_packet_1.png)

**第二种情况**，接收端只收到一个数据包，但是这一个数据包中包含了发送端发送的两个数据包的信息，这种现象即为粘包。这种情况由于接收端不知道这两个数据包的界限，所以对于接收端来说很难处理。

![tcp_packet_2](..\pic\tcp_packet_2.png)

**第三种情况**，这种情况有两种表现形式，如下图。接收端收到了两个数据包，但是这两个数据包要么是不完整的，要么就是多出来一块，这种情况即发生了拆包和粘包。这两种情况如果不加特殊处理，对于接收端同样是不好处理的。

![tcp_packet_3](..\pic\tcp_packet_3.png)



**Q：为什么会发生TCP粘包、拆包？**

- 要发送的数据大于TCP发送缓冲区剩余空间大小（由接收端窗口决定，拥塞控制），将会进行拆包
- 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包
- 要发送的数据小于TCP发送缓冲区的大小，TCP将**多次写入缓冲区的数据一次发送出去**，将会发生粘包
- 接收端应用层没有及时读取接收缓冲区中的数据，将会发生粘包



**Q：粘包、拆包解决办法**

由于 TCP 本身是面向字节流的，无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，根据业界的主流协议的解决方案，归纳如下

- **消息定长**：发送端将每个数据包封装为固定长度（不够的可以通过补 0 填充），这样接收端每次接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。
- **设置消息边界**：服务端从网络流中按消息边界分离出消息内容。在包尾增加回车换行符进行分割，例如 FTP 协议。
- **消息头+消息体结构**：消息头中包含表示消息总长度（或者消息体长度）的字段；收到包时，先接收固定字节数的头部，解出这个包完整长度，按照此长度接收包体。（这是目前网络应用中用的最多）











